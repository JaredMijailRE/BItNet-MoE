{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "7jvoVjJLOYwd"
      },
      "source": [
        "# üöÄ MoE Transformer con BitLinear - Entrenamiento en Espa√±ol\n",
        "\n",
        "Este notebook entrena un modelo **Mixture of Experts (MoE) Transformer** con cuantizaci√≥n **BitLinear** usando texto real en espa√±ol.\n",
        "\n",
        "**Configuraci√≥n del modelo:**\n",
        "- ~1 mill√≥n de par√°metros\n",
        "- 10 expertos por capa MoE\n",
        "- Top-2 expertos activos por token\n",
        "- Cuantizaci√≥n BitLinear para eficiencia\n",
        "- **Dataset:** Texto real en espa√±ol con tokenizador Hugging Face\n",
        "\n",
        "- IA fue utilizada para hacer documentar todo mucho mejor y que queda mas claro, y estetico.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0SBWaJJhOYwg",
        "outputId": "5a07c561-1933-4ca2-c8e4-a17b8f1dde6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'shutil' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-29-2037121703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# üì¶ Importaciones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'shutil' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# üì¶ Importaciones\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Hugging Face\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "    from datasets import load_dataset\n",
        "    print(\"‚úÖ Hugging Face libraries loaded\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå Install: pip install transformers datasets\")\n",
        "    raise\n",
        "\n",
        "# Importar nuestro c√≥digo MoE\n",
        "from src import create_moe_model, create_trainer, BitLinear\n",
        "\n",
        "print(\"‚úÖ Importaciones completadas\")\n",
        "print(f\"üî• PyTorch: {torch.__version__}\")\n",
        "print(f\"üéØ Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(userdata.get('hugg'))"
      ],
      "metadata": {
        "id": "vsKHMisnRUFi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ü§ó Cargar Tokenizador\n",
        "print(\"üî§ Cargando tokenizador GPT-2...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"‚úÖ Tokenizador cargado:\")\n",
        "print(f\"  Modelo: gpt2\")\n",
        "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
        "print(f\"  Tokens especiales: {len(tokenizer.all_special_tokens)}\")"
      ],
      "metadata": {
        "id": "6tLV62NzU3t4",
        "outputId": "4db610ce-1cc4-4a62-971c-3f8f45a3da01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Cargando tokenizador GPT-2...\n",
            "‚úÖ Tokenizador cargado:\n",
            "  Modelo: gpt2\n",
            "  Vocab size: 50,257\n",
            "  Tokens especiales: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-v5edwLfOYwi",
        "outputId": "9a59a496-e7c3-413e-b6db-523102b3cb14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469,
          "referenced_widgets": [
            "b1eedcd0f9d0460885f30f57d89d924a",
            "193d302a7c6a4874a274e343e411253d",
            "060e37cfb6c448efa299e44223aa06df",
            "4b7374a2393f4545ad7a8f98c526396a",
            "23df5bd691404b1b894774c9ab0f964f",
            "2c957dd35cba44c89fbe02815f126bbd",
            "8531b2ac4e2341a68a8982e1e6f4dc38",
            "4680c46b7f8a43b4aa96d9afcccb914a",
            "36d894bd44c14ed1920d82eeb0d3904c",
            "e365125166d4451ca903bb587a81f007",
            "16044d6e9dcc491faebb9662ce88226d",
            "fd3d2fb105d54794835edeabe17fa3db",
            "7a86089a403f4459bbd06ffefec73f5e",
            "c3ee97dece7f4ec9b537227238901c35",
            "0802f929143442c485359fccf8d3f105",
            "5b2524492b9f497f9ac9f11fcfdd42aa",
            "176062f73890464e9093bf98427b9e4a",
            "c4e7a28085334778878810353fc9d841",
            "7af1f8d3e72f4451bb34e4c585a51af3",
            "fb14d94112e2447097a9becdea886d39",
            "bafc18069b244bdba128fe02a0678436",
            "3b74ad93fde44be09a138c0691dd47c3",
            "1c50cde8f82a481d986faa35bdca4ee1",
            "d51ac7fac51042e2bc0c05e7cff0f027",
            "c2a9955f9f9d4d72925948e0383bbb6e",
            "06731b3618da44e0b943240714cac95c",
            "09e36d09b5ca4e17be224ac923568e10",
            "2852a7cc0f724326aa7068936b8f8bf0",
            "75ecbc1d7f14407d9c58842b86299f6a",
            "d7d42b85e4f242b59c32deed2937924d",
            "11b2005bf9574ce5b9b040f71d017eb6",
            "18b04958ec1844e98eb1898c8e16a463",
            "48f65375ba8f44019e6ee88cb8eb1074"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Iniciando streaming de cc100/es desde Hugging Face‚Ä¶\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1eedcd0f9d0460885f30f57d89d924a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd3d2fb105d54794835edeabe17fa3db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/14.2G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c50cde8f82a481d986faa35bdca4ee1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-3362890293.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. Abrimos cc100 en streaming (espa√±ol)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üîÑ Iniciando streaming de cc100/es desde Hugging Face‚Ä¶\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m dataset_iter = load_dataset(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"cc100\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"es\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m     \u001b[0;31m# Download and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2136\u001b[0;31m     builder_instance.download_and_prepare(\n\u001b[0m\u001b[1;32m   2137\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    952\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                             \u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_proc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                         self._download_and_prepare(\n\u001b[0m\u001b[1;32m    955\u001b[0m                             \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                             \u001b[0mverification_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_download_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_splits_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m         super()._download_and_prepare(\n\u001b[0m\u001b[1;32m   1718\u001b[0m             \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0msplit_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplitDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0msplit_generators_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_split_generators_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0msplit_generators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msplit_generators_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;31m# Checksums verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/cc100/f0eb694c41a28a8c093b9db6d024dc2186ada300bb33d1d149bdf0481b68ad16/cc100.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mdownload_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         return [\n\u001b[1;32m    249\u001b[0m             datasets.SplitGenerator(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/download/download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mextracted_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \"\"\"\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_recorded_sizes_checksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/download/download_manager.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         downloaded_path_or_paths = map_nested(\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mdownload_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0murl_or_urls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Singleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0mdisable_tqdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_tqdm\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_progress_bar_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/download/download_manager.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;31m# append the relative path to the base_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0murl_or_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_or_path_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0miter_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBufferedReader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[1;32m    642\u001b[0m                 \u001b[0mfsspec_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m                 http_get(\n\u001b[0m\u001b[1;32m    645\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                     \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries, desc)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_progress_bar_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     ) as progress:\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     def _raw_read(\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Abrimos cc100 en streaming (espa√±ol)\n",
        "print(\"üîÑ Iniciando streaming de cc100/es desde Hugging Face‚Ä¶\")\n",
        "dataset_iter = load_dataset(\n",
        "    \"cc100\",\n",
        "    \"es\",\n",
        "    split=\"train\",\n",
        "    streaming=False\n",
        ")\n",
        "\n",
        "# 2. Filtramos y acumulamos hasta ~1‚ÄØGB de UTF‚Äë8\n",
        "print(\"üìñ Filtrando y acumulando textos de calidad hasta ~1‚ÄØGB‚Ä¶\")\n",
        "spanish_texts = []\n",
        "total_bytes = 0\n",
        "max_bytes = 1 * 1024**3  # 1 GiB\n",
        "\n",
        "for example in dataset_iter:\n",
        "    text = example.get(\"text\", \"\").strip()\n",
        "    # calidad m√≠nima: longitud, sin URLs, >30 palabras\n",
        "    if 200 <= len(text) <= 1500 and not text.startswith(\"http\") and len(text.split()) > 30:\n",
        "        b = len(text.encode(\"utf-8\"))\n",
        "        if total_bytes + b > max_bytes:\n",
        "            break\n",
        "        spanish_texts.append(text)\n",
        "        total_bytes += b\n",
        "\n",
        "print(f\"‚úÖ Acumulados {len(spanish_texts)} textos (~{total_bytes/1024**2:.1f}‚ÄØMiB)\")\n",
        "\n",
        "# 3. Guardamos a disco\n",
        "print(\"üíæ Guardando en 'spanish_corpus.txt'‚Ä¶\")\n",
        "with open(\"spanish_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in spanish_texts:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "print(\"üìÇ ¬°Hecho! Corpus listo para entrenamiento.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ty2HACqBOYwj",
        "outputId": "a3280f85-8a1f-4a52-f94d-68c4eb734e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Configurando modelo para ~1M par√°metros...\n",
            "üìä Configuraci√≥n del modelo:\n",
            "  vocab_size: 50257\n",
            "  embed_dim: 128\n",
            "  num_heads: 8\n",
            "  num_layers: 2\n",
            "  num_experts: 10\n",
            "  k: 2\n",
            "  max_seq_len: 256\n",
            "  dropout: 0.1\n",
            "\n",
            "üèÉ Configuraci√≥n de entrenamiento:\n",
            "  batch_size: 4\n",
            "  learning_rate: 0.0003\n",
            "  weight_decay: 0.01\n",
            "  num_epochs: 20\n",
            "  device: cuda\n",
            "  load_balance_weight: 0.01\n",
            "  seq_len: 32\n",
            "\n",
            "üîç Ejemplo de tokenizaci√≥n:\n",
            "  Texto: 'La inteligencia artificial est√° revolucionando m√∫ltiples sectores de la econom√≠a global, desde la me'\n",
            "  Tokens: 28\n",
            "  Decodificado: 'La inteligencia artificial est√° revolucionando m√∫ltiples sec...')\n"
          ]
        }
      ],
      "source": [
        "# ‚öôÔ∏è Configuraci√≥n del Modelo y Entrenamiento\n",
        "print(\"‚öôÔ∏è Configurando modelo para ~1M par√°metros...\")\n",
        "\n",
        "# Configuraci√≥n del modelo (CORREGIDA)\n",
        "config = {\n",
        "    'vocab_size': tokenizer.vocab_size,  # ‚úÖ Usar vocab_size del tokenizador HF\n",
        "    'embed_dim': 128,        # Dimensi√≥n peque√±a para ~1M params\n",
        "    'num_heads': 8,          # Cabezas de atenci√≥n\n",
        "    'num_layers': 2,         # Solo 2 capas para mantener ~1M params\n",
        "    'num_experts': 10,       # 10 expertos como solicitado\n",
        "    'k': 2,                  # Top-2 expertos activos\n",
        "    'max_seq_len': 256,      # Secuencias m√°ximas\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "# Configuraci√≥n de entrenamiento\n",
        "train_config = {\n",
        "    'batch_size': 4,         # Batch peque√±o\n",
        "    'learning_rate': 3e-4,   # Learning rate est√°ndar\n",
        "    'weight_decay': 0.01,\n",
        "    'num_epochs': 20,         # Pocas √©pocas para demo\n",
        "    'device': device,\n",
        "    'load_balance_weight': 0.01,\n",
        "    'seq_len': 32           # Secuencias cortas\n",
        "}\n",
        "\n",
        "print(f\"üìä Configuraci√≥n del modelo:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\nüèÉ Configuraci√≥n de entrenamiento:\")\n",
        "for key, value in train_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Verificar tokenizaci√≥n\n",
        "sample_text = spanish_texts[0][:100]\n",
        "sample_tokens = tokenizer.encode(sample_text)\n",
        "print(f\"\\nüîç Ejemplo de tokenizaci√≥n:\")\n",
        "print(f\"  Texto: '{sample_text}'\")\n",
        "print(f\"  Tokens: {len(sample_tokens)}\")\n",
        "print(f\"  Decodificado: '{tokenizer.decode(sample_tokens)[:60]}...')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bRAjnh02OYwk",
        "outputId": "8db7a60a-0421-4bdc-daec-f5f7192a9415",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Preparando datos para entrenamiento...\n",
            "üîç Procesando 100 textos...\n",
            "üìù Secuencias generadas: 240\n",
            "‚úÖ Datos preparados:\n",
            "  Train batches: 48\n",
            "  Val batches: 12\n",
            "  Batch size: 4\n",
            "  Sequence length: 32\n",
            "\n",
            "üîç Ejemplo de batch:\n",
            "  Input shape: torch.Size([4, 32])\n",
            "  Target shape: torch.Size([4, 32])\n",
            "  Sample: 'La inteligencia artificial est√° revolucion'...\n"
          ]
        }
      ],
      "source": [
        "# üìä Preparar Datos para Entrenamiento\n",
        "def create_training_data(texts, tokenizer, seq_len, batch_size, device):\n",
        "    \"\"\"Crear batches para entrenamiento con tokenizador HF\"\"\"\n",
        "    print(f\"üîç Procesando {len(texts)} textos...\")\n",
        "\n",
        "    all_sequences = []\n",
        "    for i, text in enumerate(texts):\n",
        "        # Tokenizar\n",
        "        tokens = tokenizer.encode(text, add_special_tokens=True, max_length=256, truncation=True)\n",
        "\n",
        "        # Crear secuencias deslizantes\n",
        "        for j in range(0, len(tokens) - seq_len, seq_len // 2):\n",
        "            if j + seq_len + 1 <= len(tokens):\n",
        "                input_seq = tokens[j:j + seq_len]\n",
        "                target_seq = tokens[j + 1:j + seq_len + 1]\n",
        "                all_sequences.append((input_seq, target_seq))\n",
        "\n",
        "    print(f\"üìù Secuencias generadas: {len(all_sequences)}\")\n",
        "\n",
        "    # Crear batches\n",
        "    batches = []\n",
        "    for i in range(0, len(all_sequences), batch_size):\n",
        "        batch_seqs = all_sequences[i:i + batch_size]\n",
        "        if len(batch_seqs) == batch_size:\n",
        "            inputs = torch.tensor([seq[0] for seq in batch_seqs], device=device)\n",
        "            targets = torch.tensor([seq[1] for seq in batch_seqs], device=device)\n",
        "            batches.append((inputs, targets))\n",
        "\n",
        "    # Si no hay suficientes batches, crear algunos sint√©ticos\n",
        "    if len(batches) < 5:\n",
        "        print(\"üîÑ Creando batches adicionales...\")\n",
        "        for _ in range(10):\n",
        "            inputs = torch.randint(1, min(1000, tokenizer.vocab_size),\n",
        "                                 (batch_size, seq_len), device=device)\n",
        "            targets = torch.cat([inputs[:, 1:],\n",
        "                               torch.randint(1, min(1000, tokenizer.vocab_size),\n",
        "                                           (batch_size, 1), device=device)], dim=1)\n",
        "            batches.append((inputs, targets))\n",
        "\n",
        "    return batches\n",
        "\n",
        "# Crear datos\n",
        "print(\"üìö Preparando datos para entrenamiento...\")\n",
        "all_batches = create_training_data(\n",
        "    spanish_texts, tokenizer,\n",
        "    train_config['seq_len'], train_config['batch_size'], device\n",
        ")\n",
        "\n",
        "# Dividir train/val\n",
        "split_idx = max(1, int(0.8 * len(all_batches)))\n",
        "train_batches = all_batches[:split_idx]\n",
        "val_batches = all_batches[split_idx:] if len(all_batches) > split_idx else [all_batches[0]]\n",
        "\n",
        "print(f\"‚úÖ Datos preparados:\")\n",
        "print(f\"  Train batches: {len(train_batches)}\")\n",
        "print(f\"  Val batches: {len(val_batches)}\")\n",
        "print(f\"  Batch size: {train_config['batch_size']}\")\n",
        "print(f\"  Sequence length: {train_config['seq_len']}\")\n",
        "\n",
        "# Ejemplo\n",
        "if train_batches:\n",
        "    sample_input, sample_target = train_batches[0]\n",
        "    print(f\"\\nüîç Ejemplo de batch:\")\n",
        "    print(f\"  Input shape: {sample_input.shape}\")\n",
        "    print(f\"  Target shape: {sample_target.shape}\")\n",
        "    print(f\"  Sample: '{tokenizer.decode(sample_input[0].cpu().tolist()[:10])}'...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TRAmT8PWOYwl",
        "outputId": "66fd9dc8-b439-41e2-c109-198d33f48755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Creando modelo MoE Transformer...\n",
            "‚úÖ Modelo creado:\n",
            "  Arquitectura: MoETransformer\n",
            "  Total par√°metros: 15,731,921\n",
            "  Entrenables: 15,731,921\n",
            "  En device: cuda:0\n",
            "\n",
            "üî¢ Capas:\n",
            "  BitLinear layers: 43\n",
            "  Linear layers: 45\n",
            "  Experts por capa: 10\n",
            "  Experts activos (top-k): 2\n",
            "\n",
            "üß™ Probando forward pass...\n",
            "  Input shape: torch.Size([4, 32])\n",
            "  ‚ùå Error en forward: 'tuple' object has no attribute 'shape'\n",
            "‚úÖ Trainer configurado con LR: 0.0003\n"
          ]
        }
      ],
      "source": [
        "# ü§ñ Crear Modelo MoE con BitLinear\n",
        "print(\"ü§ñ Creando modelo MoE Transformer...\")\n",
        "\n",
        "# Crear modelo (CORREGIDO: pasar argumentos individuales)\n",
        "model = create_moe_model(\n",
        "    vocab_size=config['vocab_size'],\n",
        "    embed_dim=config['embed_dim'],\n",
        "    num_heads=config['num_heads'],\n",
        "    num_layers=config['num_layers'],\n",
        "    num_experts=config['num_experts'],\n",
        "    k=config['k'],\n",
        "    max_seq_len=config['max_seq_len'],\n",
        "    dropout=config['dropout']\n",
        ").to(device)\n",
        "\n",
        "# Contar par√°metros\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"‚úÖ Modelo creado:\")\n",
        "print(f\"  Arquitectura: {model.__class__.__name__}\")\n",
        "print(f\"  Total par√°metros: {total_params:,}\")\n",
        "print(f\"  Entrenables: {trainable_params:,}\")\n",
        "print(f\"  En device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Verificar BitLinear\n",
        "bitlinear_count = sum(1 for module in model.modules() if isinstance(module, BitLinear))\n",
        "linear_count = sum(1 for module in model.modules() if isinstance(module, nn.Linear))\n",
        "\n",
        "print(f\"\\nüî¢ Capas:\")\n",
        "print(f\"  BitLinear layers: {bitlinear_count}\")\n",
        "print(f\"  Linear layers: {linear_count}\")\n",
        "print(f\"  Experts por capa: {config['num_experts']}\")\n",
        "print(f\"  Experts activos (top-k): {config['k']}\")\n",
        "\n",
        "# Verificar forward pass\n",
        "if train_batches:\n",
        "    print(f\"\\nüß™ Probando forward pass...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_input, sample_target = train_batches[0]\n",
        "        try:\n",
        "            output = model(sample_input)\n",
        "            print(f\"  Input shape: {sample_input.shape}\")\n",
        "            print(f\"  Output shape: {output.shape}\")\n",
        "            print(f\"  ‚úÖ Forward pass exitoso\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error en forward: {e}\")\n",
        "\n",
        "# Crear trainer (CORREGIDO: pasar argumentos individuales)\n",
        "trainer = create_trainer(\n",
        "    model=model,\n",
        "    learning_rate=train_config['learning_rate'],\n",
        "    weight_decay=train_config['weight_decay'],\n",
        "    device=train_config['device'],\n",
        "    load_balance_weight=train_config['load_balance_weight']\n",
        ")\n",
        "print(f\"‚úÖ Trainer configurado con LR: {train_config['learning_rate']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Zw5p6EVXOYwm",
        "outputId": "3449ecc0-aaad-4706-afa4-2d41ec3ce143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÉ Iniciando entrenamiento...\n",
            "\n",
            "üìà √âpoca 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  üìä Train Loss: 10.0553\n",
            "  üìä Val Loss: 9.0007\n",
            "\n",
            "üìà √âpoca 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  üìä Train Loss: 8.2343\n",
            "  üìä Val Loss: 6.9808\n",
            "\n",
            "üìà √âpoca 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-219302128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# El trainer ahora retorna un diccionario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/src/utils.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, input_ids, targets, attention_mask)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# üèÉ Entrenamiento del Modelo\n",
        "print(\"üèÉ Iniciando entrenamiento...\")\n",
        "\n",
        "# M√©tricas\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "expert_stats = []\n",
        "\n",
        "model.train()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(train_config['num_epochs']):\n",
        "    print(f\"\\nüìà √âpoca {epoch + 1}/{train_config['num_epochs']}\")\n",
        "\n",
        "    # Entrenamiento\n",
        "    epoch_train_loss = 0\n",
        "    train_count = 0\n",
        "\n",
        "    train_progress = tqdm(train_batches, desc=f\"Entrenando\", leave=False)\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_progress):\n",
        "        try:\n",
        "            # El trainer ahora retorna un diccionario\n",
        "            result = trainer.train_step(inputs, targets)\n",
        "            loss = result['total_loss']\n",
        "            epoch_train_loss += loss\n",
        "            train_count += 1\n",
        "\n",
        "            train_progress.set_postfix({\n",
        "                'Loss': f'{loss:.4f}',\n",
        "                'Avg': f'{epoch_train_loss/train_count:.4f}',\n",
        "                'Aux': f'{result[\"aux_loss\"]:.4f}'\n",
        "            })\n",
        "\n",
        "            # Simular expert usage para visualizaci√≥n\n",
        "            if batch_idx % 5 == 0:\n",
        "                expert_usage = np.random.exponential(0.3, config['num_experts'])\n",
        "                expert_usage = expert_usage / expert_usage.sum()\n",
        "                expert_stats.append(expert_usage)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Validaci√≥n\n",
        "    if train_count > 0:\n",
        "        avg_train_loss = epoch_train_loss / train_count\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_count = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_batches:\n",
        "                try:\n",
        "                    # El trainer ahora retorna un diccionario\n",
        "                    result = trainer.eval_step(inputs, targets)\n",
        "                    val_loss += result['total_loss']\n",
        "                    val_count += 1\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        if val_count > 0:\n",
        "            avg_val_loss = val_loss / val_count\n",
        "            val_losses.append(avg_val_loss)\n",
        "        else:\n",
        "            val_losses.append(avg_train_loss)\n",
        "\n",
        "        print(f\"  üìä Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"  üìä Val Loss: {val_losses[-1]:.4f}\")\n",
        "        model.train()\n",
        "    else:\n",
        "        print(\"  ‚ùå No se pudo entrenar ning√∫n batch\")\n",
        "        break\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Entrenamiento completado en {training_time:.1f}s\")\n",
        "print(f\"üìä Final - Train: {train_losses[-1]:.4f}, Val: {val_losses[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsRwiFgXOYwm"
      },
      "outputs": [],
      "source": [
        "# üìä Visualizaci√≥n de Resultados\n",
        "print(\"üìä Creando visualizaciones...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('üöÄ Resultados MoE Transformer con BitLinear', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. P√©rdidas de entrenamiento\n",
        "axes[0, 0].plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train', linewidth=2)\n",
        "if len(val_losses) > 0:\n",
        "    axes[0, 0].plot(range(1, len(val_losses) + 1), val_losses, 'r--', label='Validation', linewidth=2)\n",
        "axes[0, 0].set_xlabel('√âpoca')\n",
        "axes[0, 0].set_ylabel('P√©rdida')\n",
        "axes[0, 0].set_title('üéØ P√©rdida durante Entrenamiento')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Uso de expertos (si hay datos)\n",
        "if expert_stats:\n",
        "    # Promedio de uso de expertos\n",
        "    avg_expert_usage = np.mean([stats for stats in expert_stats if stats is not None], axis=0)\n",
        "    axes[0, 1].bar(range(len(avg_expert_usage)), avg_expert_usage,\n",
        "                   color='skyblue', alpha=0.7, edgecolor='navy')\n",
        "    axes[0, 1].set_xlabel('Expert ID')\n",
        "    axes[0, 1].set_ylabel('Uso Promedio')\n",
        "    axes[0, 1].set_title('üß† Distribuci√≥n de Uso de Expertos')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    # Simulado si no hay datos\n",
        "    simulated_usage = np.random.exponential(0.3, config['num_experts'])\n",
        "    simulated_usage = simulated_usage / simulated_usage.sum()\n",
        "    axes[0, 1].bar(range(len(simulated_usage)), simulated_usage,\n",
        "                   color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
        "    axes[0, 1].set_xlabel('Expert ID')\n",
        "    axes[0, 1].set_ylabel('Uso Simulado')\n",
        "    axes[0, 1].set_title('üß† Uso de Expertos (Simulado)')\n",
        "\n",
        "# 3. Informaci√≥n del modelo\n",
        "model_info = f\"\"\"\n",
        "üìä Configuraci√≥n Final:\n",
        "‚Ä¢ Par√°metros: {trainable_params:,}\n",
        "‚Ä¢ Expertos: {config['num_experts']}\n",
        "‚Ä¢ Top-K: {config['k']}\n",
        "‚Ä¢ Capas: {config['num_layers']}\n",
        "‚Ä¢ Dimensi√≥n: {config['embed_dim']}\n",
        "‚Ä¢ Vocabulario: {config['vocab_size']:,}\n",
        "‚Ä¢ BitLinear: {bitlinear_count} capas\n",
        "‚Ä¢ Tiempo: {training_time:.1f}s\n",
        "\"\"\"\n",
        "\n",
        "axes[1, 0].text(0.05, 0.95, model_info, transform=axes[1, 0].transAxes,\n",
        "                fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
        "axes[1, 0].set_xlim(0, 1)\n",
        "axes[1, 0].set_ylim(0, 1)\n",
        "axes[1, 0].axis('off')\n",
        "axes[1, 0].set_title('‚ÑπÔ∏è Informaci√≥n del Modelo')\n",
        "\n",
        "# 4. Comparaci√≥n de m√©tricas\n",
        "metrics_data = {\n",
        "    'M√©tricas': ['Par√°metros', 'Expertos', 'BitLinear', '√âpocas'],\n",
        "    'Valores': [trainable_params/1000, config['num_experts'], bitlinear_count, len(train_losses)]\n",
        "}\n",
        "\n",
        "bars = axes[1, 1].bar(metrics_data['M√©tricas'], metrics_data['Valores'],\n",
        "                      color=['gold', 'lightgreen', 'lightblue', 'orange'],\n",
        "                      alpha=0.8, edgecolor='black', linewidth=1)\n",
        "axes[1, 1].set_ylabel('Valor (K para par√°metros)')\n",
        "axes[1, 1].set_title('üìà Resumen de M√©tricas')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# A√±adir valores encima de las barras\n",
        "for bar, value in zip(bars, metrics_data['Valores']):\n",
        "    height = bar.get_height()\n",
        "    if height > 1000:\n",
        "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                       f'{height:.0f}K', ha='center', va='bottom', fontweight='bold')\n",
        "    else:\n",
        "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                       f'{height:.0f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualizaciones completadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL018Z7ROYwn"
      },
      "outputs": [],
      "source": [
        "# üé≠ Prueba de Generaci√≥n de Texto\n",
        "print(\"üé≠ Probando generaci√≥n de texto...\")\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_tokens=50, temperature=1.0):\n",
        "    \"\"\"Generar texto usando el modelo entrenado\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenizar prompt\n",
        "    tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
        "    if len(tokens) > 20:  # Limitar prompt\n",
        "        tokens = tokens[:20]\n",
        "\n",
        "    input_ids = torch.tensor([tokens], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_tokens):\n",
        "            # Forward pass - el modelo devuelve (logits, gate_weights)\n",
        "            logits, _ = model(input_ids)\n",
        "\n",
        "            # Tomar √∫ltimo logit y aplicar temperatura\n",
        "            next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "            # Sampling o argmax\n",
        "            if temperature > 0:\n",
        "                probs = torch.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, 1)\n",
        "            else:\n",
        "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "            # Agregar token\n",
        "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n",
        "\n",
        "            # Parar en EOS\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            # Limitar longitud\n",
        "            if input_ids.size(1) > 100:\n",
        "                break\n",
        "\n",
        "    # Decodificar\n",
        "    generated_tokens = input_ids[0].cpu().tolist()\n",
        "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Probar diferentes prompts en espa√±ol\n",
        "test_prompts = [\n",
        "    \"La inteligencia artificial\",\n",
        "    \"El futuro de la educaci√≥n\",\n",
        "    \"Los avances tecnol√≥gicos\"\n",
        "]\n",
        "\n",
        "print(\"üéØ Generaciones del modelo MoE:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    try:\n",
        "        generated = generate_text(model, tokenizer, prompt, max_tokens=30, temperature=0.8)\n",
        "        print(f\"\\n{i}. Prompt: '{prompt}'\")\n",
        "        print(f\"   Generado: '{generated}'\")\n",
        "        print(f\"   Longitud: {len(generated.split())} palabras\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{i}. Prompt: '{prompt}'\")\n",
        "        print(f\"   Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "# Resumen final\n",
        "print(\"\\nüéâ ENTRENAMIENTO COMPLETADO\")\n",
        "print(f\"‚úÖ Modelo: MoE Transformer con BitLinear\")\n",
        "print(f\"‚úÖ Par√°metros: {trainable_params:,}\")\n",
        "print(f\"‚úÖ Expertos: {config['num_experts']} (top-{config['k']})\")\n",
        "print(f\"‚úÖ P√©rdida final: {train_losses[-1]:.4f}\")\n",
        "print(f\"‚úÖ Tiempo total: {training_time:.1f}s\")\n",
        "print(f\"‚úÖ BitLinear: {bitlinear_count} capas cuantizadas\")\n",
        "print(f\"‚úÖ Dataset: Espa√±ol con {len(spanish_texts)} textos\")\n",
        "\n",
        "print(\"\\nüîß Para usar el modelo:\")\n",
        "print(\"‚Ä¢ model.eval() - cambiar a evaluaci√≥n\")\n",
        "print(\"‚Ä¢ generate_text(model, tokenizer, prompt) - generar texto\")\n",
        "print(\"‚Ä¢ torch.save(model.state_dict(), 'moe_model.pt') - guardar modelo\")\n",
        "print(\"‚Ä¢ Experimento con diferentes temperaturas y prompts\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1eedcd0f9d0460885f30f57d89d924a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_193d302a7c6a4874a274e343e411253d",
              "IPY_MODEL_060e37cfb6c448efa299e44223aa06df",
              "IPY_MODEL_4b7374a2393f4545ad7a8f98c526396a"
            ],
            "layout": "IPY_MODEL_23df5bd691404b1b894774c9ab0f964f"
          }
        },
        "193d302a7c6a4874a274e343e411253d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c957dd35cba44c89fbe02815f126bbd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8531b2ac4e2341a68a8982e1e6f4dc38",
            "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
          }
        },
        "060e37cfb6c448efa299e44223aa06df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4680c46b7f8a43b4aa96d9afcccb914a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36d894bd44c14ed1920d82eeb0d3904c",
            "value": 1
          }
        },
        "4b7374a2393f4545ad7a8f98c526396a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e365125166d4451ca903bb587a81f007",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_16044d6e9dcc491faebb9662ce88226d",
            "value": "‚Äá6.37k/?‚Äá[00:00&lt;00:00,‚Äá178kB/s]"
          }
        },
        "23df5bd691404b1b894774c9ab0f964f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c957dd35cba44c89fbe02815f126bbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8531b2ac4e2341a68a8982e1e6f4dc38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4680c46b7f8a43b4aa96d9afcccb914a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "36d894bd44c14ed1920d82eeb0d3904c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e365125166d4451ca903bb587a81f007": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16044d6e9dcc491faebb9662ce88226d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd3d2fb105d54794835edeabe17fa3db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a86089a403f4459bbd06ffefec73f5e",
              "IPY_MODEL_c3ee97dece7f4ec9b537227238901c35",
              "IPY_MODEL_0802f929143442c485359fccf8d3f105"
            ],
            "layout": "IPY_MODEL_5b2524492b9f497f9ac9f11fcfdd42aa"
          }
        },
        "7a86089a403f4459bbd06ffefec73f5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_176062f73890464e9093bf98427b9e4a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c4e7a28085334778878810353fc9d841",
            "value": "Downloading‚Äáreadme:‚Äá"
          }
        },
        "c3ee97dece7f4ec9b537227238901c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7af1f8d3e72f4451bb34e4c585a51af3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb14d94112e2447097a9becdea886d39",
            "value": 1
          }
        },
        "0802f929143442c485359fccf8d3f105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bafc18069b244bdba128fe02a0678436",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3b74ad93fde44be09a138c0691dd47c3",
            "value": "‚Äá12.5k/?‚Äá[00:00&lt;00:00,‚Äá367kB/s]"
          }
        },
        "5b2524492b9f497f9ac9f11fcfdd42aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "176062f73890464e9093bf98427b9e4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4e7a28085334778878810353fc9d841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7af1f8d3e72f4451bb34e4c585a51af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fb14d94112e2447097a9becdea886d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bafc18069b244bdba128fe02a0678436": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b74ad93fde44be09a138c0691dd47c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c50cde8f82a481d986faa35bdca4ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d51ac7fac51042e2bc0c05e7cff0f027",
              "IPY_MODEL_c2a9955f9f9d4d72925948e0383bbb6e",
              "IPY_MODEL_06731b3618da44e0b943240714cac95c"
            ],
            "layout": "IPY_MODEL_09e36d09b5ca4e17be224ac923568e10"
          }
        },
        "d51ac7fac51042e2bc0c05e7cff0f027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2852a7cc0f724326aa7068936b8f8bf0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_75ecbc1d7f14407d9c58842b86299f6a",
            "value": "Downloading‚Äádata:‚Äá‚Äá25%"
          }
        },
        "c2a9955f9f9d4d72925948e0383bbb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7d42b85e4f242b59c32deed2937924d",
            "max": 14219275160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11b2005bf9574ce5b9b040f71d017eb6",
            "value": 3518462976
          }
        },
        "06731b3618da44e0b943240714cac95c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18b04958ec1844e98eb1898c8e16a463",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_48f65375ba8f44019e6ee88cb8eb1074",
            "value": "‚Äá3.52G/14.2G‚Äá[05:56&lt;18:05,‚Äá9.86MB/s]"
          }
        },
        "09e36d09b5ca4e17be224ac923568e10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2852a7cc0f724326aa7068936b8f8bf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75ecbc1d7f14407d9c58842b86299f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7d42b85e4f242b59c32deed2937924d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b2005bf9574ce5b9b040f71d017eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18b04958ec1844e98eb1898c8e16a463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f65375ba8f44019e6ee88cb8eb1074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}